100 эпох, 2 скрытых слоя по 35 нейронов, tanh и relu

Adadelta - нет сходимости на 100 эпохах (отриц. R2), требуется несколько тысяч эпох, результат все равно хуже Adam

Adagrad - нестабильные результаты, большой разброс R2

Adam - стабильно R2 > 0.97, быстрая сходимость (40 эпох) - с relu. С tanh хуже: R2 < 0.97

Adamax - почти как Adam, но чуть хуже

Ftrl - нет сходимости на 100 эпохах (отриц. R2), требуется несколько тысяч эпох, результат все равно хуже Adam

Nadam - R2 сопоставима с Adam, но иногда присутствуют пульсации, сходимость визуально медленнее

RMSprop - чуть хуже Adamax, есть пульсации

SGD - R2 < 0.96, сходимость медленнее, чем у Adam

У всех оптимизаторов результаты лучше с relu, чем с tanh

Лидер - Adam, 2-е место - Adamax
